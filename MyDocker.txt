In VM we do virtualization using hypervisor.
Docker is the sotware that allows to run container.
It is not good idea to host DB in contaier. If cotainer is dead then we will loose all the data. Hence, we just host the application.
We cannot run os in detached mode as OS is plain and there will be no process running in it. There should be atleast a process running to run image as container.
When we run image then it becomes container.Otw, it remains as image.
Image has a layered structure like Centos, java, tomcat etc.
We can launch a container in a interactive ot detached mode.
Images are immutable. we cannot edit any image but allows us to take a image, run as container and save as new image. 

Commands:
service docker status ---> Tells whether docker is up and running
docker pull nginx ---> To pull rom docker hub
docker images ---> List out the pulled images
docker run -d -P nginx ---> To run container
docker ps ---> List out the running containers
docker exec -it cdf8f3b48d2b cat /etc/os-release ---> To check the details of the nginx server. Also, to enetr into the running container. 
docker stop nginx ---> graceul stop but stop and kill do the same. just stops the contaier
docker kill nginx ---> stops abruptly
docker history ngnix ---> is the command by using which we can see how image is created.
docker cp is command to copy file from container to local vice versa.

If someone removes running container after doing some changes all our changes will be lost. We can create our own image as below with the changes we done.
Creating a new image from running container:
Since the container is already running, we can only enter into the container using the command docker exec <contaier_id> <command to execute>
Then save the container as a new image. docker commit -m "new file created" <container_id> <new_image_name>
By doing above, if someone remove the old container, since we already have a new image ready, we can launch a container.
This is how we can create a new own image in manual process.

We can create a new image by pulling lite wait OS, run in interactive mode and save it as new image or refer Creating new image section:

Images can be created in 2 ways:
1. Manual: (We need to build up our image manually. Below is the manual process of creating a image).
	We need to write requirement as below:
		centos: Launch centos in interactive mode.
		java: do changes like install java, tomcat etc.
		tomcat: save as a new image. When the container is saved, it creates a new image. We can now launch a new contaier using the newly created image.
2. Automation:






Creating new Image:
1. Create a Dockerfile as below and save it. By doing below, we will get updated ubuntu image with curl & vim installed.
FROM ubuntu (First or base layer of the image)
MAINTAINER "Naresh"
RUN apt-get update
RUN apt-get install -y curl vim

-------------------------------------------------
Below should be given in jenkins shell while creating job to automate the creation of Docker file.
sudo rm /tmp/naresh/Dockerfile
sudo rm /tmp/naresh/addressbook.war
sudo touch /tmp/naresh/Dockerfile
sudo chown jenkins:jenkins /tmp/naresh/Dockerfile
sudo cp target/addressbook.war /tmp/naresh/
sudo chown jenkins:jenkins /tmp/naresh/addressbook.war
sudo cat <<EOT>> /tmp/naresh/Dockerfile
FROM tomcat
ADD addressbook.war /usr/local/tomcat/webapps
CMD "catalina.sh" "run"
EXPOSE 8080
EOT
sudo chmod 777 /tmp/naresh/Dockerfile
sudo chmod 777 /tmp/naresh/addressbook.war

2. docker build is the command using which we can create a new image using Dockerfile; -t means tag
docker build -t ubuntu:v2 <current_path> (or)
docker build -t ubuntu:v2 -f <docker_custom_filename> <current_path>

This is how we can create a new image with out creating a container or entering into it.

We need to rename the image to push into docker hub: docker tag <image_name> <dockerhub_userid>/<repository_name>:v1
Then push to docker hub. First login to docker using command docker login and then push using the command docker push <dockerhub_userid>/<repository_name>:v1
In organisations, we use docker trusted repositories to upload images so that they are not accessible to outside world.

If we stop cotainer using stop or kill some one can restart them back and there is a chance of running old code if we restart them back. so better we use docker rm <contaier_id> if we do not need that contaier at all.


we cannot change image once it is created. rather, run the image as contaier and once the contaier is up and running, then make your changes and create it as a new image


Creating Dockerfile:
FROM:
For beginners it’s enough to understand that every Dockerfile must start with the FROM instruction in the form of FROM <image>[:tag]. This will set the base image for your Dockerfile, which means that subsequent instructions will be applied to this base image.

The tag value is optional, if you don’t specify the tag Docker will use the tag latest and will try and use or pull the latest version of the base image during build.

On the little bit more advanced side, let’s note the following:

There is one instruction that you can put before FROM into your Dockerfile. This instruction is ARG. ARG is used to specify arguments for the docker build command with the --build-arg <varname>=<value> flag.
You can have more than one FROM instructions in your Dockerfile. You will want to use this feature, for example, when you use one base image to build your app and another base image to run it.
It’s called a multi-stage build and you can read about it here.

This is why every section that starts with FROM in your Dockerfile is called a build stage (even in the simple case of having only one FROM instruction). You can specify the name of the build stage in the form FROM <image>[:tag] [AS <name>].

COPY vs ADD:
Both ADD and COPY are designed to add directories and files to your Docker image in the form of ADD <src>... <dest> or COPY <src>... <dest>. Most resources, including myself, suggest to use COPY.

The reason behind this is that ADD has extra features compared to COPY that make ADD more unpredictable and a bit over-designed. ADD can pull files from url sources, which COPY cannot. ADD can also extract compressed files assuming it can recognize and handle the format. You cannot extract archives with COPY.

The ADD instruction was added to Docker first, and COPY was added later to provide a straightforward, rock solid solution for copying files and directories into your container’s file system.

If you want to pull files from the web into your image I would suggest to use RUN and curl and uncompress your files with RUN and commands you would use on the command line.

ENV:
ENV is used to define environment variables. The interesting thing about ENV is that it does two things:

You can use it to define environment variables that will be available in your container. So when you build an image and start up a container with that image you’ll find that the environment variable is available and is set to the value you specified in the Dockerfile.
You can use the variables that you specify by ENV in the Dockerfile itself. So in subsequent instructions the environment variable will be available.

RUN
RUN will execute commands, so it’s one of the most-used instructions. I would like to highlight two points:

You’ll use a lot of apt-get type of commands to add new packages to your image. It’s always advisable to put apt-get update and apt-get install commands on the same line. This is important because of layer caching. Having these on two separate lines would mean that if you add a new package to your install list, the layer with apt-get update will not be invalidated in the layer cache and you might end up in a mess. Read more here.
RUN has two forms; RUN <command> (called shell form) and RUN ["executable", "param1", "param2"] called exec form. Please note that RUN <command> will invoke a shell automatically (/bin/sh -c by default), while the exec form will not invoke a command shell. If you want to tackle a problem around this read here.

VOLUME
This is where I found Docker documentation not so easy to follow. So let me put it in plain English.

You can use the VOLUME instruction in a Dockerfile to tell Docker that the stuff you store in that specific directory should be stored on the host file system not in the container file system. This implies that stuff stored in the volume will persist and be available also after you destroy the container.

In other words it is best practice to crate a volume for your data files, database files, or any file or directory that your users will change when they use your application.

The data stored in the volume will remain on the host machine even if you stop the container and remove the container with docker rm. (The volume will be removed on exit if you start the container with docker run --rm, though.)

You can also share these volumes between containers with docker run --volumes-from.

You can inspect your volumes with the docker volume ls and docker volume inspect commands.

You can also have a look inside your volumes by navigating to Docker volumes in your file system. On Linux you can go to /var/lib/docker/volumes pick the id of the volume and list it as a directory. You can find out the id of the container and thus the volume by running docker inspect on your container.

On Mac, you’ll not be able to access /var/lib/docker/volumes so easily. If you run screen ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/tty on your Mac, you get a terminal where you can navigate as if you were using a Linux machine.

The difference between the VOLUME instruction in Dockerfile and starting your container with docker run -v ... is this: VOLUME in Dockerfile will create a new empty directory for your files under the standard Docker structure, i.e. /var/lib/docker/volumes. docker run -v ... can do more, you can mount existing directories from your host file system into your container and you can also specify the path of the directory on the host.

Now you may think that docker run -v ... is the better option, but keep in mind that mounting an existing directory assumes that the directory exists on the host, which may give you portability issues.

This topic is a good candidate for another detailed post and video, so let’s move on for now.

USER
Don’t run your stuff as root, be humble, use the USER instruction to specify the user. This user will be used to run any subsequent RUN, CMD AND ENDPOINT instructions in your Dockerfile.

WORKDIR
A very convenient way to define the working directory, it will be used with subsequent RUN, CMD, ENTRYPOINT, COPY and ADD instructions. You can specify WORKDIR multiple times in a Dockerfile.

If the directory does not exists, Docker will create it for you.

EXPOSE
An important instruction to inform your users about the ports your application is listening on. EXPOSE will not publish the port, you need to use docker run -p... to do that when you start the container.

CMD and ENTRYPOINT
CMD is the instruction to specify what component is to be run by your image with arguments in the following form: CMD [“executable”, “param1”, “param2”…].

You can override CMD when you’re starting up your container by specifying your command after the image name like this: $ docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...].

You can only specify one CMD in a Dockerfile (OK, physically you can specify more than one, but only the last one will be used).

It is good practice to specify a CMD even if you are developing a generic container, in this case an interactive shell is a good CMD entry. So you do CMD ["python"] or CMD [“php”, “-a”] to give your users something to work with.

So what’s the deal with ENTRYPOINT? When you specify an entry point, your image will work a bit differently. You use ENTRYPOINT as the main executable of your image. In this case whatever you specify in CMD will be added to ENTRYPOINT as parameters.

ENTRYPOINT ["git"]
CMD ["--help"]
This way you can build Docker images that mimic the behavior of the main executable you specify in ENTRYPOINT.

ONBUILD
This is so nice. You can specify instructions with ONBUILD that will be executed when your image is used as the base image of another Dockerfile. :)

This is useful when you want to create a generic base image to be used in different variations by many Dockerfiles, or in many projects or by many parties.

So you do not need to add the specific stuff immediately, like you don’t need to copy the source code or config files in the base image. How could you even do that, when these things will be available only later?

So what you do instead is to add ONBUILD instructions. So you can do something like this:

ONBUILD COPY . /usr/src/app
ONBUILD RUN /usr/src/app/mybuild.sh
ONBUILD instructions will be executed right after the FROM instruction in the downstram Dockerfile.

Docker Volume:
Volume is helpful to share the data from local to container & vice versa.

Suppose, we have a machine we need to install OS, docker and then create an image. Suppose, we have a data inside a container and if contaier is hung or crashed all of a suddenn. In that case we cannot use docker cp also to get the data from that contaier. So, there should be a action to preserve the data as there will be some important config files. We need a permanent machanism so that what ever the data that is getting generated in the container and if we can copy  that to local machine then even container crashed then our data will be secured. 

Whatever the folder we are creating on the host machine to save the data from the contaier is called as volume. Volume is nothing but the mount thet we create on our local machine where we have to transfer data from container to volume. It works 2ways: we can transer data from container to host or vice versa.

docker volume create my-vol

After creating above volume, it sits at /var/lib/docker/volume

We can run a container in the particular volume. Suppose if we create a file under volume folder and attach that particular volume to a container, those files will be shared to container. We cannot attach volume to a contaier once it is launched. So, the best practice is launch a container along with the volume.

docker run -d -P -v <volume_name>:<Path_where_we_need_to_copy_the_data_to_container> <image_name>

Other way of creating volume:
Create 2 separate index.html in 2 different folder once containing Welcome to dev & Welcome to prod env. To display 2 different messages in dev & prod we need not create 2 diefferent images to display different messages. We can use same image but when we launch it as a container, we can replace the required files, parameters during the container invocation.

docker run -d -P --name devngnix -v  <Path_where_folders_are_created>:<Path_where_we_need_to_copy_the_data_to_container> <image_name>

Here we need not create volume, we can directly map the folder created on local machine.

Docker Compose:
Docker Compose is a utility using which we can run a multiple contaiers at a time. Compose follows a declrative syntax. By deault, compose has the name docker-compose.yml

create a yml file and use following command to launch a contaier.
docker-compose -f <yml_file_name> up --scale web=4(no.of times to launch container) -d

We can scale down the containers as well. Just change the number web=2. It automatically scales down the remaining containers.

To run multiple images as contaiers use following command:
docker-compose -f <yml_file_name> -p webapps up -d --scale web=4 --scale app=4

webapp is the name that is given to the complete above stack

Dis-advantages of compose:
When we run multiple containeres, they all run on single machine. Resource utilization is very high and if that machine goes down we cannot access that application. In organisations it is not the right way of running containers in production. To overcome this Docker Swarm comes into the picture. 

Docker Swarm:
It is a orchestrization mechanism in docker which means it manages our contaiers or run containes in a better way. 

To have high availability of containers, we need to scale up or env to multiple nodes. We need to take multiple server machines and all these machines connect to the main node. All this forms as a cluster which is called as Docker Swarm. 

Creating communication between containers:
Create the first container that will act as the server-sshserver

# docker run -i -t -p 2222:22 --name sshserver ubuntu bash
Create a second container that acts like an SSH client

# docker run -i -t --name sshclient --link  sshserver:sshserver ubuntu bash
Docker--link creates private channels between containers

inspect your linked container with docker inspect

~# docker inspect -f "{{ .HostConfig.Links }}" sshclient[/sshserver:/sshclient/sshserver]
SSH into the SSH server with its IP

#ssh root@172.17.0.3 -p 22
